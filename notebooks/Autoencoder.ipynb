{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "from beerai.config import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors():\n",
    "    vec_file = os.path.join(DATA_DIR, \"processed/recipe_vecs.h5\")\n",
    "    with pd.HDFStore(vec_file, \"r\") as store:\n",
    "        vectors = store.get(\"/vecs\")\n",
    "    return vectors\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_file = os.path.join(DATA_DIR, \"processed/vocab.pickle\")\n",
    "    with open(vocab_file, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    inv_vocab = {v: k for k,v in vocab.items()}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = load_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>...</th>\n",
       "      <th>678.0</th>\n",
       "      <th>679.0</th>\n",
       "      <th>680.0</th>\n",
       "      <th>681.0</th>\n",
       "      <th>682.0</th>\n",
       "      <th>683.0</th>\n",
       "      <th>684.0</th>\n",
       "      <th>685.0</th>\n",
       "      <th>686.0</th>\n",
       "      <th>boil_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.274902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.553145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.240266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name            0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  ...  678.0  \\\n",
       "recipe_id                                                         ...          \n",
       "0          0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1          0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "5          0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7          1.553145  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "8          1.240266  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "name       679.0  680.0  681.0     682.0  683.0  684.0  685.0  686.0  \\\n",
       "recipe_id                                                              \n",
       "0            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "1            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "5            0.0    0.0    0.0  1.274902    0.0    0.0    0.0    0.0   \n",
       "7            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "8            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "\n",
       "name       boil_time  \n",
       "recipe_id             \n",
       "0               60.0  \n",
       "1               60.0  \n",
       "5               90.0  \n",
       "7               60.0  \n",
       "8               60.0  \n",
       "\n",
       "[5 rows x 688 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes[\"boil_time\"] = recipes.boil_time.clip(upper=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "recipes_scaled = scaler.fit_transform(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on sparse autoencoders\n",
    "\n",
    "- [Discussion on quora](https://www.quora.com/When-training-an-autoencoder-on-very-sparse-data-how-do-you-force-the-decoder-to-reconstruct-mostly-zeros-rather-than-always-just-reconstructing-the-average)\n",
    "  - > So, if you're including a bias term in your autoencoders, I recommend removing the bias and attempting training again.\n",
    "  - > I encountered this problem recently. I found that, similar to what Eric described, it is a problem with SGD getting stuck in bad local optima. A couple things helped: (1) using conjugate gradients or AdaGrad, either of which will find the path to the true minimum without getting stuck as much as plain SGD will; (2) using a combined cross-entropy & mean-squared-error loss function (assuming that you can model your data as binary vectors or as probability distributions) pulls things -- ever so slightly -- in the right directions better than either alone would.\n",
    "  - > I decided to progressively lower the learning rate and then I got good results. So that's my tip, lower your learning rate until you get better results. (For example, I'm using now: 0.0000005 as the initial rate.) And donâ€™t forget to normalize your data!\n",
    "  - > You shouldn't need to do anything special for this. Standard good practices for initialization and training should take care of it.\n",
    "  - > We were able to reproduce the original image (not get the average) by using AdamOptimizer and lowering the learning rate.\n",
    "  - [Notes](http://web.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf) from Stanford CS294a (Andrew Ng)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        compression_dim,\n",
    "        factor_per_layer,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.compression_dim = compression_dim\n",
    "\n",
    "        comp_layers, decomp_layers = self.gen_layers_by_factor(\n",
    "            input_dim, compression_dim, factor_per_layer\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(*comp_layers)\n",
    "        self.decoder = nn.Sequential(*decomp_layers)\n",
    " \n",
    "    def gen_layers_by_factor(self, input_dim, compression_dim, factor_per_layer):\n",
    "        \n",
    "        cur_dim = input_dim\n",
    "        compress_layers = []\n",
    "        decompress_layers = []\n",
    "        n_iters = math.ceil(math.log(input_dim / compression_dim, factor_per_layer))\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            new_dim = max(cur_dim // factor_per_layer, compression_dim)\n",
    "            compress_layers.extend([nn.Linear(cur_dim, new_dim, bias=False), nn.ReLU(True)])\n",
    "            decompress_layers.extend([nn.ReLU(True), nn.Linear(new_dim, cur_dim, bias=False)])\n",
    "            cur_dim = new_dim\n",
    "        decompress_layers = decompress_layers[::-1]\n",
    "        # Replace final layer with sigmoid/tanh. Should match the input scaling range \n",
    "        compress_layers[-1] = nn.Sigmoid()\n",
    "        decompress_layers[-1] = nn.Sigmoid()\n",
    "\n",
    "        return compress_layers, decompress_layers\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self.encoder(X)\n",
    "        return pd.DataFrame(X.detach().numpy())\n",
    "    \n",
    "    def decode(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self.decoder(X)\n",
    "        return pd.DataFrame(X.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, batch_size, num_epochs, learning_rate=1e-3, beta=1):\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    kldiv = nn.KLDivLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-5\n",
    "    )\n",
    "\n",
    "    #features = torch.tensor(X, dtype=torch.float)\n",
    "    data = torch.tensor(X, dtype=torch.float)\n",
    "    \n",
    "    #train = data_utils.TensorDataset(features)\n",
    "    \n",
    "    # Shuffle used to ensure randomized selection\n",
    "    #train_loader = data_utils.DataLoader(\n",
    "    #    train, batch_size=batch_size, shuffle=True\n",
    "    #)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        #for i, batch in enumerate(train_loader):\n",
    "            # ===================forward=====================\n",
    "            #data = batch[0]\n",
    "        decoded = model.forward(data)\n",
    "        length = decoded.shape[0]\n",
    "\n",
    "        if length < batch_size:\n",
    "            continue\n",
    "        mse_loss = mse(decoded, data)\n",
    "        kld_loss = kldiv(decoded, data)\n",
    "        loss = mse_loss + beta * kld_loss\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "\n",
    "        print(f\"epoch [{epoch + 1}/{num_epochs}], loss:{sum(losses)/len(losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = len(recipes.columns)\n",
    "# first guess\n",
    "compress_dims = 50\n",
    "# factor to reduce by each layer\n",
    "factor_per_layer = 2\n",
    "beer_ae = AutoEncoder(input_dims, compress_dims, factor_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-rory/repos/beer.ai/env/lib/python3.8/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.249381\n",
      "epoch [2/100], loss:0.247609\n",
      "epoch [3/100], loss:0.245421\n",
      "epoch [4/100], loss:0.242263\n",
      "epoch [5/100], loss:0.237759\n",
      "epoch [6/100], loss:0.231570\n",
      "epoch [7/100], loss:0.223319\n",
      "epoch [8/100], loss:0.212647\n",
      "epoch [9/100], loss:0.199245\n",
      "epoch [10/100], loss:0.182827\n",
      "epoch [11/100], loss:0.163280\n",
      "epoch [12/100], loss:0.140818\n",
      "epoch [13/100], loss:0.116047\n",
      "epoch [14/100], loss:0.090176\n",
      "epoch [15/100], loss:0.065023\n",
      "epoch [16/100], loss:0.042750\n",
      "epoch [17/100], loss:0.025224\n",
      "epoch [18/100], loss:0.013237\n",
      "epoch [19/100], loss:0.006186\n",
      "epoch [20/100], loss:0.002592\n",
      "epoch [21/100], loss:0.000970\n",
      "epoch [22/100], loss:0.000305\n",
      "epoch [23/100], loss:0.000052\n",
      "epoch [24/100], loss:-0.000042\n",
      "epoch [25/100], loss:-0.000079\n",
      "epoch [26/100], loss:-0.000097\n",
      "epoch [27/100], loss:-0.000104\n",
      "epoch [28/100], loss:-0.000104\n",
      "epoch [29/100], loss:-0.000100\n",
      "epoch [30/100], loss:-0.000099\n",
      "epoch [31/100], loss:-0.000101\n",
      "epoch [32/100], loss:-0.000105\n",
      "epoch [33/100], loss:-0.000106\n",
      "epoch [34/100], loss:-0.000104\n",
      "epoch [35/100], loss:-0.000101\n",
      "epoch [36/100], loss:-0.000103\n",
      "epoch [37/100], loss:-0.000105\n",
      "epoch [38/100], loss:-0.000106\n",
      "epoch [39/100], loss:-0.000104\n",
      "epoch [40/100], loss:-0.000103\n",
      "epoch [41/100], loss:-0.000104\n",
      "epoch [42/100], loss:-0.000106\n",
      "epoch [43/100], loss:-0.000106\n",
      "epoch [44/100], loss:-0.000105\n",
      "epoch [45/100], loss:-0.000104\n",
      "epoch [46/100], loss:-0.000105\n",
      "epoch [47/100], loss:-0.000106\n",
      "epoch [48/100], loss:-0.000106\n",
      "epoch [49/100], loss:-0.000105\n",
      "epoch [50/100], loss:-0.000105\n",
      "epoch [51/100], loss:-0.000106\n",
      "epoch [52/100], loss:-0.000106\n",
      "epoch [53/100], loss:-0.000105\n",
      "epoch [54/100], loss:-0.000105\n",
      "epoch [55/100], loss:-0.000106\n",
      "epoch [56/100], loss:-0.000106\n",
      "epoch [57/100], loss:-0.000106\n",
      "epoch [58/100], loss:-0.000106\n",
      "epoch [59/100], loss:-0.000106\n",
      "epoch [60/100], loss:-0.000106\n",
      "epoch [61/100], loss:-0.000106\n",
      "epoch [62/100], loss:-0.000106\n",
      "epoch [63/100], loss:-0.000106\n",
      "epoch [64/100], loss:-0.000106\n",
      "epoch [65/100], loss:-0.000106\n",
      "epoch [66/100], loss:-0.000106\n",
      "epoch [67/100], loss:-0.000106\n",
      "epoch [68/100], loss:-0.000106\n",
      "epoch [69/100], loss:-0.000106\n",
      "epoch [70/100], loss:-0.000106\n",
      "epoch [71/100], loss:-0.000106\n",
      "epoch [72/100], loss:-0.000106\n",
      "epoch [73/100], loss:-0.000106\n",
      "epoch [74/100], loss:-0.000106\n",
      "epoch [75/100], loss:-0.000106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b7932f03cdcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# weight of KL loss term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeer_ae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipes_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#train(beer_ae, recipes.values, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d471eaaf00b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, batch_size, num_epochs, learning_rate, beta)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmse_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mkld_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkldiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkld_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/beer.ai/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/beer.ai/env/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/beer.ai/env/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mkl_div\u001b[0;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[1;32m   2406\u001b[0m             \u001b[0mreduction_enum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2408\u001b[0;31m     \u001b[0mreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'batchmean'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "num_epochs=100\n",
    "learning_rate=1e-3\n",
    "# weight of KL loss term\n",
    "beta = 0.1\n",
    "train(beer_ae, recipes_scaled, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)\n",
    "#train(beer_ae, recipes.values, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = beer_ae.decode(beer_ae.encode(recipes.values).values)\n",
    "decoded.index = recipes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded[decoded < 1e-6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404601</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404604</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404606</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404622</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404634</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.604214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171700 rows Ã— 688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1    2    3    4    5    6    7    8    9    ...  678  679  \\\n",
       "recipe_id                                                    ...             \n",
       "0          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "5          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "7          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "8          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "404601     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "404604     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "404606     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "404622     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "404634     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "           680  681  682  683  684  685  686       687  \n",
       "recipe_id                                               \n",
       "0          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604217  \n",
       "1          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604218  \n",
       "5          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.605101  \n",
       "7          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604216  \n",
       "8          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604217  \n",
       "...        ...  ...  ...  ...  ...  ...  ...       ...  \n",
       "404601     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604216  \n",
       "404604     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604217  \n",
       "404606     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604217  \n",
       "404622     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604217  \n",
       "404634     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.604214  \n",
       "\n",
       "[171700 rows x 688 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11336676649631938"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((recipes.values - decoded.values)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer = beer_ae.decode(np.random.rand(50))\n",
    "random_beer = scaler.inverse_transform(random_beer.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer[random_beer < 0.0001] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_ids = np.where(random_beer > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  18,  19,  20,  21,  22,  23,  24,  25,  26,  28,\n",
       "        29,  30,  31,  32,  33,  34,  35,  36,  38,  39,  40,  41,  42,\n",
       "        43,  44,  45,  48,  49,  51,  52,  53,  54,  55,  56,  57,  59,\n",
       "        60,  61,  62,  63,  64,  66,  67,  68,  69,  71,  72,  73,  74,\n",
       "        76,  77,  78,  79,  81,  82,  83,  84,  85,  86,  88,  89,  92,\n",
       "        93,  94,  95, 100, 103, 104, 106, 107, 109, 110, 111, 112, 113,\n",
       "       116, 117, 118, 120, 125, 129, 130, 131, 135, 141, 142, 143, 145,\n",
       "       149, 150, 152, 153, 154, 155, 156, 157, 159, 161, 164, 166, 167,\n",
       "       178, 180, 181, 182, 185, 186, 187, 188, 193, 196, 199, 201, 205,\n",
       "       209, 210, 211, 212, 213, 218, 219, 220, 224, 225, 228, 229, 230,\n",
       "       231, 235, 236, 237, 240, 242, 248, 252, 253, 254, 256, 258, 260,\n",
       "       261, 267, 269, 271, 273, 275, 277, 278, 282, 284, 285, 287, 296,\n",
       "       297, 308, 317, 320, 322, 329, 331, 334, 343, 345, 366, 384, 397,\n",
       "       398, 403, 431, 435, 437, 439, 450, 475, 478, 480, 483, 484, 487,\n",
       "       497, 501, 505, 507, 508, 512, 513, 517, 518, 525, 526, 529, 540,\n",
       "       541, 542, 546, 549, 552, 558, 563, 565, 572, 575, 576, 577, 578,\n",
       "       579, 580, 586, 589, 590, 592, 594, 595, 596, 597, 598, 601, 605,\n",
       "       607, 608, 609, 610, 612, 613, 615, 619, 621, 622, 623, 624, 625,\n",
       "       626, 627, 628, 629, 631, 633, 636, 637, 638, 639, 641, 642, 644,\n",
       "       645, 646, 647, 648, 649, 650, 651, 652, 653, 655, 656, 657, 658,\n",
       "       659, 660, 661, 662, 663, 664, 665, 666, 667, 676, 677, 678, 679,\n",
       "       680, 681, 682, 683, 684, 686, 687])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ferm_2-row\n",
      "ferm_6-row\n",
      "ferm_abbey malt\n",
      "ferm_acidulated malt\n",
      "ferm_amber malt\n",
      "ferm_aromatic barley malt\n",
      "ferm_aromaticÂ® (munich) malt\n",
      "ferm_biscuitÂ® mdâ„¢\n",
      "ferm_black malt\n",
      "ferm_blackprinz\n",
      "ferm_brown malt\n",
      "ferm_brown sugar\n",
      "ferm_brown sugar, dark\n",
      "ferm_brown sugar, light\n",
      "ferm_candi sugar, amber\n",
      "ferm_candi sugar, clear\n",
      "ferm_cane sugar\n",
      "ferm_cara malt\n",
      "ferm_caraamber\n",
      "ferm_caraaroma\n",
      "ferm_carabelge\n",
      "ferm_caracrystal wheat malt\n",
      "ferm_carafa i\n",
      "ferm_carafa ii\n",
      "ferm_carafa iii\n",
      "ferm_carafa special iii\n",
      "ferm_carafoam\n",
      "ferm_carahellÂ®\n",
      "ferm_caramel malt 10l\n",
      "ferm_caramel malt 120l\n",
      "ferm_caramel malt 20l\n",
      "ferm_caramel malt 40l\n",
      "ferm_caramel malt 80l\n",
      "ferm_caramel/crystal 150l\n",
      "ferm_caramel/crystal 60l\n",
      "ferm_caramel/crystal 90l\n",
      "ferm_caramunich\n",
      "ferm_caramunich i\n",
      "ferm_caramunich ii\n",
      "ferm_caramunich iii\n",
      "ferm_carapilsÂ®Â malt\n",
      "ferm_carared\n",
      "ferm_chocolate\n",
      "ferm_chocolate malt\n",
      "ferm_chocolate wheat\n",
      "ferm_coffee malt\n",
      "ferm_corn sugar (dextrose)\n",
      "ferm_crystal malt\n",
      "ferm_crystal rye\n",
      "ferm_dark chocolate\n",
      "ferm_dark dry malt extract\n",
      "ferm_dry malt extract - amber\n",
      "ferm_dry malt extract - light\n",
      "ferm_dry malt extract - pilsen\n",
      "ferm_dry malt extract - wheat\n",
      "ferm_extra special malt\n",
      "ferm_flaked barley\n",
      "ferm_flaked oats\n",
      "ferm_flaked rice\n",
      "ferm_flaked rye\n",
      "ferm_flaked wheat\n",
      "ferm_golden naked oats\n",
      "ferm_golden promise\n",
      "ferm_honey\n",
      "ferm_honey malt\n",
      "ferm_liquid malt extract - amber\n",
      "ferm_liquid malt extract - dark\n",
      "ferm_liquid malt extract - light\n",
      "ferm_liquid malt extract - munich\n",
      "ferm_liquid malt extract - wheat\n",
      "ferm_maltodextrine\n",
      "ferm_maple syrup\n",
      "ferm_maris otter lme\n",
      "ferm_maris otter pale\n",
      "ferm_melanoidin\n",
      "ferm_milk sugar (lactose)\n",
      "ferm_molasses\n",
      "ferm_munich malt\n",
      "ferm_munich malt - 20l\n",
      "ferm_munich malt 10l\n",
      "ferm_munich type i\n",
      "ferm_pale malt\n",
      "ferm_pilsen malt\n",
      "ferm_pumpkin, cooked\n",
      "ferm_rice hulls\n",
      "ferm_roasted barley\n",
      "ferm_rye malt\n",
      "ferm_smoked malt\n",
      "ferm_special b\n",
      "ferm_special roast\n",
      "ferm_sugar, table (sucrose)\n",
      "ferm_victoryÂ®Â malt\n",
      "ferm_vienna\n",
      "ferm_warminster floor-malted maris otter\n",
      "ferm_white wheat\n",
      "hop_admiral\n",
      "hop_amarillo\n",
      "hop_apollo\n",
      "hop_aramis\n",
      "hop_azacca\n",
      "hop_bramling cross\n",
      "hop_bravo\n",
      "hop_brewers gold\n",
      "hop_buzz bullets\n",
      "hop_calypso\n",
      "hop_cascade\n",
      "hop_centennial\n",
      "hop_challenger\n",
      "hop_chinook\n",
      "hop_citra\n",
      "hop_cluster\n",
      "hop_columbus\n",
      "hop_crystal\n",
      "hop_delta\n",
      "hop_east kent golding\n",
      "hop_el dorado\n",
      "hop_ella (stella)\n",
      "hop_falconer's flight\n",
      "hop_fuggle\n",
      "hop_galaxy\n",
      "hop_galena\n",
      "hop_glacier\n",
      "hop_green bullet\n",
      "hop_hallertau\n",
      "hop_hallertau blanc\n",
      "hop_heirloom/homegrown\n",
      "hop_hersbrucker\n",
      "hop_huell melon\n",
      "hop_jarrylo\n",
      "hop_kohatu\n",
      "hop_liberty\n",
      "hop_loral\n",
      "hop_magnum\n",
      "hop_mandarina bavaria\n",
      "hop_marynka\n",
      "hop_mosaicâ„¢\n",
      "hop_motueka\n",
      "hop_mount hood\n",
      "hop_nelson sauvin\n",
      "hop_new zealand fuggle\n",
      "hop_newport\n",
      "hop_northdown\n",
      "hop_northern brewer\n",
      "hop_nugget\n",
      "hop_pacific gem\n",
      "hop_pacific jade\n",
      "hop_pacifica\n",
      "hop_perle\n",
      "hop_pilgrim\n",
      "hop_progress\n",
      "hop_saaz\n",
      "hop_santiam\n",
      "hop_saphir\n",
      "hop_simcoe\n",
      "hop_smaragd\n",
      "hop_sorachi ace\n",
      "hop_southern cross\n",
      "hop_sterling\n",
      "hop_strisselspalt\n",
      "hop_styrian bobek\n",
      "hop_styrian golding\n",
      "hop_summit\n",
      "hop_super pride\n",
      "hop_super styrians\n",
      "hop_target\n",
      "hop_tettnanger\n",
      "hop_tnt\n",
      "hop_tradition\n",
      "hop_warrior\n",
      "hop_willamette\n",
      "hop_amarillo_dry\n",
      "hop_belma_dry\n",
      "hop_bramling cross_dry\n",
      "hop_brewers gold_dry\n",
      "hop_cascade_dry\n",
      "hop_centennial_dry\n",
      "hop_citra_dry\n",
      "hop_east kent golding_dry\n",
      "hop_el dorado_dry\n",
      "hop_hallertau_dry\n",
      "hop_kohatu_dry\n",
      "hop_mosaicâ„¢_dry\n",
      "hop_motueka_dry\n",
      "hop_nelson sauvin_dry\n",
      "hop_saaz_dry\n",
      "hop_simcoe_dry\n",
      "hop_smaragd_dry\n",
      "hop_sorachi ace_dry\n",
      "hop_styrian bobek_dry\n",
      "hop_warrior_dry\n",
      "hop_zythos_dry\n",
      "yeast_abbey ale\n",
      "yeast_american ale\n",
      "yeast_american ale ii\n",
      "yeast_american wheat\n",
      "yeast_belgian lambic blend\n",
      "yeast_belgian strong ale\n",
      "yeast_belgian wit ale\n",
      "yeast_belle saison\n",
      "yeast_bohemian lager\n",
      "yeast_british ale\n",
      "yeast_british ale ii\n",
      "yeast_burton union\n",
      "yeast_california ale\n",
      "yeast_czech pils\n",
      "yeast_danish lager\n",
      "yeast_denny's favorite 50\n",
      "yeast_forbidden fruit\n",
      "yeast_french saison\n",
      "yeast_german ale\n",
      "yeast_german wheat\n",
      "yeast_irish ale\n",
      "yeast_lactobacillus\n",
      "yeast_london esb ale\n",
      "yeast_newcastle dark ale\n",
      "yeast_nottingham ale\n",
      "yeast_premium gold\n",
      "yeast_safale s-04\n",
      "yeast_safale us-05\n",
      "yeast_safbrew s-33\n",
      "yeast_safbrew t-58\n",
      "yeast_safbrew wb-06\n",
      "yeast_saflager s-23\n",
      "yeast_super high gravity ale (wlp099)\n",
      "yeast_trappist high gravity\n",
      "yeast_u.s. west coast\n",
      "yeast_vermont ale\n",
      "yeast_weihenstephan weizen\n",
      "yeast_west yorkshire (1469)\n",
      "yeast_whitbread ale\n",
      "yeast_windsor\n",
      "yeast_workhorse\n",
      "misc_allspice\n",
      "misc_apricot puree\n",
      "misc_bitter orange peel\n",
      "misc_black pepper\n",
      "misc_blackberries\n",
      "misc_blueberries\n",
      "misc_bourbon soaked french oak chips\n",
      "misc_brown sugar\n",
      "misc_calcium carbonate\n",
      "misc_cardamom seed\n",
      "misc_cherries\n",
      "misc_chocolate\n",
      "misc_cinnamon\n",
      "misc_cinnamon sticks\n",
      "misc_cocoa nibs\n",
      "misc_cocoa powder\n",
      "misc_coffee\n",
      "misc_coffee grounds\n",
      "misc_coriander seed\n",
      "misc_dextrose\n",
      "misc_espresso\n",
      "misc_french oak cubes\n",
      "misc_gelatin\n",
      "misc_ginger root\n",
      "misc_grains of paradise\n",
      "misc_gypsum\n",
      "misc_heather tips\n",
      "misc_honey\n",
      "misc_irish moss\n",
      "misc_juniper berries\n",
      "misc_lactic acid\n",
      "misc_lactose\n",
      "misc_lavender\n",
      "misc_lemon juice\n",
      "misc_lemon zest\n",
      "misc_lemongrass\n",
      "misc_licorice root\n",
      "misc_maltodextrin\n",
      "misc_mango\n",
      "misc_maple syrup\n",
      "misc_nutmeg\n",
      "misc_oak chips\n",
      "misc_orange juice\n",
      "misc_orange zest\n",
      "misc_peaches\n",
      "misc_priming sugar\n",
      "misc_pumpkin\n",
      "misc_pumpkin pie spice\n",
      "misc_raisins\n",
      "misc_raspberry\n",
      "misc_star anise\n",
      "misc_strawberries\n",
      "misc_sweet orange peel\n",
      "misc_toasted coconut\n",
      "misc_vanilla beans\n",
      "misc_vanilla extract\n",
      "misc_whirlfloc tablet\n",
      "misc_whiskey\n",
      "misc_whole cloves\n",
      "misc_yeast nutrient\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "687",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ab2939d4ece9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ming_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 687"
     ]
    }
   ],
   "source": [
    "for ing in ing_ids:\n",
    "    print(inv_vocab[ing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas from Ethan\n",
    "\n",
    "\n",
    "## Model Ideas\n",
    "\n",
    "* Look at adding regularization in between layers\n",
    "* Look at VAE\n",
    "* Look at GAN\n",
    "  * This is most interesting\n",
    "  * Its explicit job is creating recipes that look real.\n",
    "  * Discriminator needs to have some function that estimates \"quality\" of what is produced (decoded space).\n",
    "\n",
    "Combining the properties of a VAE (where you can interpolate) and a GAN (where you can estimate \"quality\" of a recipe) would be perfect.\n",
    "\n",
    "Main issue is that reconstruction error doesn't give you the properties you want. Look for other learning signals - what other aspect of a recipe is meaningful? Could we use the beer style as a label?\n",
    "\n",
    "\n",
    "## Encoding Ideas\n",
    "\n",
    "* Can you encode hops into all possible combinations present in the dataset (not all combinations period)? Rob thinks this is reasonable\n",
    "* Can we make ingredient \"categories\"? e.g. caramel with varying lovabond measurement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beer.ai",
   "language": "python",
   "name": "beer.ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
