{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "from beerai.config import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors():\n",
    "    vec_file = os.path.join(DATA_DIR, \"processed/recipe_vecs.h5\")\n",
    "    with pd.HDFStore(vec_file, \"r\") as store:\n",
    "        vectors = store.get(\"/vecs\")\n",
    "    return vectors\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_file = os.path.join(DATA_DIR, \"processed/vocab.pickle\")\n",
    "    with open(vocab_file, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    inv_vocab = {v: k for k,v in vocab.items()}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = load_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>...</th>\n",
       "      <th>678.0</th>\n",
       "      <th>679.0</th>\n",
       "      <th>680.0</th>\n",
       "      <th>681.0</th>\n",
       "      <th>682.0</th>\n",
       "      <th>683.0</th>\n",
       "      <th>684.0</th>\n",
       "      <th>685.0</th>\n",
       "      <th>686.0</th>\n",
       "      <th>boil_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.274902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.553145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.240266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name            0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  ...  678.0  \\\n",
       "recipe_id                                                         ...          \n",
       "0          0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1          0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "5          0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7          1.553145  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "8          1.240266  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "name       679.0  680.0  681.0     682.0  683.0  684.0  685.0  686.0  \\\n",
       "recipe_id                                                              \n",
       "0            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "1            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "5            0.0    0.0    0.0  1.274902    0.0    0.0    0.0    0.0   \n",
       "7            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "8            0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "\n",
       "name       boil_time  \n",
       "recipe_id             \n",
       "0               60.0  \n",
       "1               60.0  \n",
       "5               90.0  \n",
       "7               60.0  \n",
       "8               60.0  \n",
       "\n",
       "[5 rows x 688 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes[\"boil_time\"] = recipes.boil_time.clip(upper=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>...</th>\n",
       "      <th>678.0</th>\n",
       "      <th>679.0</th>\n",
       "      <th>680.0</th>\n",
       "      <th>681.0</th>\n",
       "      <th>682.0</th>\n",
       "      <th>683.0</th>\n",
       "      <th>684.0</th>\n",
       "      <th>685.0</th>\n",
       "      <th>686.0</th>\n",
       "      <th>boil_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>565.699829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name              0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  ...  \\\n",
       "recipe_id                                                           ...   \n",
       "16004      565.699829  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "name       678.0  679.0  680.0  681.0  682.0  683.0  684.0  685.0  686.0  \\\n",
       "recipe_id                                                                  \n",
       "16004        0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "name       boil_time  \n",
       "recipe_id             \n",
       "16004           60.0  \n",
       "\n",
       "[1 rows x 688 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.loc[recipes.loc[:,0]==recipes.loc[:,0].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_file = os.path.join(DATA_DIR, \"interim/all_recipes.h5\")\n",
    "with pd.HDFStore(recipes_file) as store:\n",
    "    recs = store.select(\"core\", where=\"index==16004\")\n",
    "    ings = store.select(\"ingredients\", where=\"index==16004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ferm_amount</th>\n",
       "      <th>ferm_color</th>\n",
       "      <th>ferm_display_amount</th>\n",
       "      <th>ferm_name</th>\n",
       "      <th>ferm_origin</th>\n",
       "      <th>ferm_potential</th>\n",
       "      <th>ferm_type</th>\n",
       "      <th>ferm_yield</th>\n",
       "      <th>hop_alpha</th>\n",
       "      <th>hop_amount</th>\n",
       "      <th>...</th>\n",
       "      <th>misc_time</th>\n",
       "      <th>misc_use</th>\n",
       "      <th>yeast_amount</th>\n",
       "      <th>yeast_attenuation</th>\n",
       "      <th>yeast_flocculation</th>\n",
       "      <th>yeast_form</th>\n",
       "      <th>yeast_laboratory</th>\n",
       "      <th>yeast_name</th>\n",
       "      <th>yeast_product_id</th>\n",
       "      <th>yeast_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>5848.388452</td>\n",
       "      <td>25.0</td>\n",
       "      <td>12893.5 lb</td>\n",
       "      <td>melanoidin</td>\n",
       "      <td>de</td>\n",
       "      <td>1.037</td>\n",
       "      <td>kilned malt</td>\n",
       "      <td>0.798964</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.014175</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dry</td>\n",
       "      <td>fermentis</td>\n",
       "      <td>safale s-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>4816.330574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10618.2 lb</td>\n",
       "      <td>carapils</td>\n",
       "      <td>de</td>\n",
       "      <td>1.035</td>\n",
       "      <td>adjunct</td>\n",
       "      <td>0.755776</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>64.546142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>142.3 lb</td>\n",
       "      <td>2-row</td>\n",
       "      <td>us</td>\n",
       "      <td>1.037</td>\n",
       "      <td>base malt</td>\n",
       "      <td>0.798964</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.017010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1365</td>\n",
       "      <td>0.022680</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ferm_amount  ferm_color ferm_display_amount   ferm_name ferm_origin  \\\n",
       "id                                                                           \n",
       "16004  5848.388452        25.0          12893.5 lb  melanoidin          de   \n",
       "16004  4816.330574         1.0          10618.2 lb    carapils          de   \n",
       "16004    64.546142         1.0            142.3 lb       2-row          us   \n",
       "16004          NaN         NaN                 NaN         NaN         NaN   \n",
       "16004          NaN         NaN                 NaN         NaN         NaN   \n",
       "\n",
       "       ferm_potential    ferm_type  ferm_yield  hop_alpha  hop_amount  ...  \\\n",
       "id                                                                     ...   \n",
       "16004           1.037  kilned malt    0.798964     0.1365    0.014175  ...   \n",
       "16004           1.035      adjunct    0.755776     0.1365    0.017010  ...   \n",
       "16004           1.037    base malt    0.798964     0.1365    0.017010  ...   \n",
       "16004             NaN          NaN         NaN     0.1365    0.017010  ...   \n",
       "16004             NaN          NaN         NaN     0.1365    0.022680  ...   \n",
       "\n",
       "      misc_time misc_use yeast_amount yeast_attenuation  yeast_flocculation  \\\n",
       "id                                                                            \n",
       "16004       NaN      NaN          NaN              70.0                 NaN   \n",
       "16004       NaN      NaN          NaN               NaN                 NaN   \n",
       "16004       NaN      NaN          NaN               NaN                 NaN   \n",
       "16004       NaN      NaN          NaN               NaN                 NaN   \n",
       "16004       NaN      NaN          NaN               NaN                 NaN   \n",
       "\n",
       "      yeast_form  yeast_laboratory   yeast_name yeast_product_id  yeast_type  \n",
       "id                                                                            \n",
       "16004        dry         fermentis  safale s-04              NaN         ale  \n",
       "16004        NaN               NaN          NaN              NaN         NaN  \n",
       "16004        NaN               NaN          NaN              NaN         NaN  \n",
       "16004        NaN               NaN          NaN              NaN         NaN  \n",
       "16004        NaN               NaN          NaN              NaN         NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recs\n",
    "ings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "recipes_scaled = scaler.fit_transform(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on sparse autoencoders\n",
    "\n",
    "- [Discussion on quora](https://www.quora.com/When-training-an-autoencoder-on-very-sparse-data-how-do-you-force-the-decoder-to-reconstruct-mostly-zeros-rather-than-always-just-reconstructing-the-average)\n",
    "  - > So, if you're including a bias term in your autoencoders, I recommend removing the bias and attempting training again.\n",
    "  - > I encountered this problem recently. I found that, similar to what Eric described, it is a problem with SGD getting stuck in bad local optima. A couple things helped: (1) using conjugate gradients or AdaGrad, either of which will find the path to the true minimum without getting stuck as much as plain SGD will; (2) using a combined cross-entropy & mean-squared-error loss function (assuming that you can model your data as binary vectors or as probability distributions) pulls things -- ever so slightly -- in the right directions better than either alone would.\n",
    "  - > I decided to progressively lower the learning rate and then I got good results. So that's my tip, lower your learning rate until you get better results. (For example, I'm using now: 0.0000005 as the initial rate.) And don’t forget to normalize your data!\n",
    "  - > You shouldn't need to do anything special for this. Standard good practices for initialization and training should take care of it.\n",
    "  - > We were able to reproduce the original image (not get the average) by using AdamOptimizer and lowering the learning rate.\n",
    "  - [Notes](http://web.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf) from Stanford CS294a (Andrew Ng)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        compression_dim,\n",
    "        factor_per_layer,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.compression_dim = compression_dim\n",
    "\n",
    "        comp_layers, decomp_layers = self.gen_layers_by_factor(\n",
    "            input_dim, compression_dim, factor_per_layer\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(*comp_layers)\n",
    "        self.decoder = nn.Sequential(*decomp_layers)\n",
    " \n",
    "    def gen_layers_by_factor(self, input_dim, compression_dim, factor_per_layer):\n",
    "        \n",
    "        cur_dim = input_dim\n",
    "        compress_layers = []\n",
    "        decompress_layers = []\n",
    "        n_iters = math.ceil(math.log(input_dim / compression_dim, factor_per_layer))\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            new_dim = max(cur_dim // factor_per_layer, compression_dim)\n",
    "            compress_layers.extend([nn.Linear(cur_dim, new_dim, bias=False), nn.ReLU(True)])\n",
    "            decompress_layers.extend([nn.ReLU(True), nn.Linear(new_dim, cur_dim, bias=False)])\n",
    "            cur_dim = new_dim\n",
    "        decompress_layers = decompress_layers[::-1]\n",
    "        # Replace final layer with sigmoid/tanh. Should match the input scaling range \n",
    "        compress_layers[-1] = nn.Sigmoid()\n",
    "        decompress_layers[-1] = nn.Sigmoid()\n",
    "\n",
    "        return compress_layers, decompress_layers\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self.encoder(X)\n",
    "        return pd.DataFrame(X.detach().numpy())\n",
    "    \n",
    "    def decode(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self.decoder(X)\n",
    "        return pd.DataFrame(X.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, batch_size, num_epochs, learning_rate=1e-3, beta=1):\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    kldiv = nn.KLDivLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-5\n",
    "    )\n",
    "\n",
    "    #features = torch.tensor(X, dtype=torch.float)\n",
    "    data = torch.tensor(X, dtype=torch.float)\n",
    "    \n",
    "    #train = data_utils.TensorDataset(features)\n",
    "    \n",
    "    # Shuffle used to ensure randomized selection\n",
    "    #train_loader = data_utils.DataLoader(\n",
    "    #    train, batch_size=batch_size, shuffle=True\n",
    "    #)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        #for i, batch in enumerate(train_loader):\n",
    "            # ===================forward=====================\n",
    "            #data = batch[0]\n",
    "        decoded = model.forward(data)\n",
    "        length = decoded.shape[0]\n",
    "\n",
    "        if length < batch_size:\n",
    "            continue\n",
    "        mse_loss = mse(decoded, data)\n",
    "        kld_loss = kldiv(decoded, data)\n",
    "        loss = (1 - beta) * mse_loss + beta * kld_loss\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "\n",
    "        print(f\"epoch [{epoch + 1}/{num_epochs}], loss:{sum(losses)/len(losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = len(recipes.columns)\n",
    "# first guess\n",
    "compress_dims = 50\n",
    "# factor to reduce by each layer\n",
    "factor_per_layer = 2\n",
    "beer_ae = AutoEncoder(input_dims, compress_dims, factor_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-rory/repos/beer.ai/env/lib/python3.8/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.966428\n",
      "epoch [2/100], loss:0.964557\n",
      "epoch [3/100], loss:0.962276\n",
      "epoch [4/100], loss:0.959147\n",
      "epoch [5/100], loss:0.954853\n",
      "epoch [6/100], loss:0.949130\n",
      "epoch [7/100], loss:0.941658\n",
      "epoch [8/100], loss:0.932106\n",
      "epoch [9/100], loss:0.920158\n",
      "epoch [10/100], loss:0.905554\n",
      "epoch [11/100], loss:0.888175\n",
      "epoch [12/100], loss:0.868150\n",
      "epoch [13/100], loss:0.846044\n",
      "epoch [14/100], loss:0.823057\n",
      "epoch [15/100], loss:0.801139\n",
      "epoch [16/100], loss:0.782606\n",
      "epoch [17/100], loss:0.769360\n",
      "epoch [18/100], loss:0.761712\n",
      "epoch [19/100], loss:0.758312\n",
      "epoch [20/100], loss:0.757227\n",
      "epoch [21/100], loss:0.757039\n",
      "epoch [22/100], loss:0.757077\n",
      "epoch [23/100], loss:0.757126\n",
      "epoch [24/100], loss:0.757138\n",
      "epoch [25/100], loss:0.757101\n",
      "epoch [26/100], loss:0.757034\n",
      "epoch [27/100], loss:0.756985\n",
      "epoch [28/100], loss:0.756980\n",
      "epoch [29/100], loss:0.756939\n",
      "epoch [30/100], loss:0.756823\n",
      "epoch [31/100], loss:0.756696\n",
      "epoch [32/100], loss:0.756614\n",
      "epoch [33/100], loss:0.756563\n",
      "epoch [34/100], loss:0.756560\n",
      "epoch [35/100], loss:0.756527\n",
      "epoch [36/100], loss:0.756505\n",
      "epoch [37/100], loss:0.756464\n",
      "epoch [38/100], loss:0.756454\n",
      "epoch [39/100], loss:0.756439\n",
      "epoch [40/100], loss:0.756435\n",
      "epoch [41/100], loss:0.756445\n",
      "epoch [42/100], loss:0.756445\n",
      "epoch [43/100], loss:0.756411\n",
      "epoch [44/100], loss:0.756350\n",
      "epoch [45/100], loss:0.756297\n",
      "epoch [46/100], loss:0.756286\n",
      "epoch [47/100], loss:0.756320\n",
      "epoch [48/100], loss:0.756297\n",
      "epoch [49/100], loss:0.756243\n",
      "epoch [50/100], loss:0.756189\n",
      "epoch [51/100], loss:0.756167\n",
      "epoch [52/100], loss:0.756168\n",
      "epoch [53/100], loss:0.756194\n",
      "epoch [54/100], loss:0.756180\n",
      "epoch [55/100], loss:0.756151\n",
      "epoch [56/100], loss:0.756132\n",
      "epoch [57/100], loss:0.756129\n",
      "epoch [58/100], loss:0.756141\n",
      "epoch [59/100], loss:0.756152\n",
      "epoch [60/100], loss:0.756145\n",
      "epoch [61/100], loss:0.756125\n",
      "epoch [62/100], loss:0.756111\n",
      "epoch [63/100], loss:0.756110\n",
      "epoch [64/100], loss:0.756117\n",
      "epoch [65/100], loss:0.756124\n",
      "epoch [66/100], loss:0.756122\n",
      "epoch [67/100], loss:0.756111\n",
      "epoch [68/100], loss:0.756103\n",
      "epoch [69/100], loss:0.756104\n",
      "epoch [70/100], loss:0.756109\n",
      "epoch [71/100], loss:0.756112\n",
      "epoch [72/100], loss:0.756108\n",
      "epoch [73/100], loss:0.756102\n",
      "epoch [74/100], loss:0.756099\n",
      "epoch [75/100], loss:0.756100\n",
      "epoch [76/100], loss:0.756103\n",
      "epoch [77/100], loss:0.756103\n",
      "epoch [78/100], loss:0.756101\n",
      "epoch [79/100], loss:0.756097\n",
      "epoch [80/100], loss:0.756096\n",
      "epoch [81/100], loss:0.756098\n",
      "epoch [82/100], loss:0.756099\n",
      "epoch [83/100], loss:0.756097\n",
      "epoch [84/100], loss:0.756095\n",
      "epoch [85/100], loss:0.756092\n",
      "epoch [86/100], loss:0.756090\n",
      "epoch [87/100], loss:0.756086\n",
      "epoch [88/100], loss:0.756075\n",
      "epoch [89/100], loss:0.756052\n",
      "epoch [90/100], loss:0.756035\n",
      "epoch [91/100], loss:0.756076\n",
      "epoch [92/100], loss:0.756047\n",
      "epoch [93/100], loss:0.756034\n",
      "epoch [94/100], loss:0.756042\n",
      "epoch [95/100], loss:0.756050\n",
      "epoch [96/100], loss:0.756053\n",
      "epoch [97/100], loss:0.756054\n",
      "epoch [98/100], loss:0.756050\n",
      "epoch [99/100], loss:0.756044\n",
      "epoch [100/100], loss:0.756036\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "num_epochs=100\n",
    "learning_rate=1e-3\n",
    "# weight of KL loss term\n",
    "beta = 0.1\n",
    "train(beer_ae, recipes_scaled, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)\n",
    "#train(beer_ae, recipes.values, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = beer_ae.decode(beer_ae.encode(recipes.values).values)\n",
    "decoded.index = recipes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded[decoded < 1e-6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>680</th>\n",
       "      <th>681</th>\n",
       "      <th>682</th>\n",
       "      <th>683</th>\n",
       "      <th>684</th>\n",
       "      <th>685</th>\n",
       "      <th>686</th>\n",
       "      <th>687</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.630809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.630807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.630808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.630808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404601</th>\n",
       "      <td>0.630809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404604</th>\n",
       "      <td>0.630809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404606</th>\n",
       "      <td>0.630809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404622</th>\n",
       "      <td>0.630808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404634</th>\n",
       "      <td>0.630808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171700 rows × 688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0    1    2    3    4    5    6    7    8    9    ...  678  \\\n",
       "recipe_id                                                         ...        \n",
       "0          0.630809  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1          0.630809  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "5          0.630807  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "7          0.630808  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "8          0.630808  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "...             ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "404601     0.630809  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404604     0.630809  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404606     0.630809  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404622     0.630808  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404634     0.630808  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "           679  680  681       682  683  684  685  686  687  \n",
       "recipe_id                                                    \n",
       "0          0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "1          0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "5          0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "7          0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "8          0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "...        ...  ...  ...       ...  ...  ...  ...  ...  ...  \n",
       "404601     0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "404604     0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "404606     0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "404622     0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "404634     0.0  0.0  0.0  0.000001  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[171700 rows x 688 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1177227152658449"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((recipes.values - decoded.values)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_beer(model):\n",
    "    latent_space_dim = model.compression_dim\n",
    "    random_beer = model.decode(np.random.rand(latent_space_dim))\n",
    "    random_beer = scaler.inverse_transform(random_beer.T)[0]\n",
    "    random_beer[random_beer < 0.0001] = 0\n",
    "    return random_beer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer = beer_ae.decode(np.random.rand(50))\n",
    "random_beer = scaler.inverse_transform(random_beer.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer[random_beer < 0.0001] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_ids = np.where(random_beer > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "[  0  32  33  34  38  44  48  60  66  78  85  92 103 107 117 120 129 149\n",
      " 150 152 154 155 157 164 180 187 202 211 230 252 256 284 297 483 558 575\n",
      " 576 645 649 682 687]\n"
     ]
    }
   ],
   "source": [
    "print(len(ing_ids))\n",
    "print(ing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7462286949157715 kg of ferm_2-row\n",
      "0.00012904341565445065 kg of ferm_caramel malt 120l\n",
      "0.0001188625319628045 kg of ferm_caramel malt 20l\n",
      "0.3263695538043976 kg of ferm_caramel malt 40l\n",
      "0.36618995666503906 kg of ferm_caramel/crystal 60l\n",
      "0.376726895570755 kg of ferm_carapils® malt\n",
      "0.00015614827862009406 kg of ferm_chocolate\n",
      "0.001890511834062636 kg of ferm_dry malt extract - light\n",
      "0.0023655961267650127 kg of ferm_flaked oats\n",
      "0.00015075632836669683 kg of ferm_liquid malt extract - light\n",
      "0.32154062390327454 kg of ferm_maris otter pale\n",
      "0.00010746826592367142 kg of ferm_munich malt\n",
      "0.37367182970046997 kg of ferm_pilsen malt\n",
      "0.0012666976545006037 kg of ferm_roasted barley\n",
      "0.00047409036778844893 kg of ferm_vienna\n",
      "0.36029115319252014 kg of ferm_white wheat\n",
      "0.3887019157409668 kg of hop_amarillo\n",
      "0.00010415908036520705 kg of hop_calypso\n",
      "0.6255908608436584 kg of hop_cascade\n",
      "0.42424145340919495 kg of hop_centennial\n",
      "0.33386945724487305 kg of hop_chinook\n",
      "0.4257890582084656 kg of hop_citra\n",
      "0.34627604484558105 kg of hop_columbus\n",
      "0.35958075523376465 kg of hop_east kent golding\n",
      "0.0002597385609988123 kg of hop_fuggle\n",
      "0.004057986196130514 kg of hop_hallertau\n",
      "0.00010068433039123192 kg of hop_jester\n",
      "0.00047229023766703904 kg of hop_magnum\n",
      "0.00010436041338834912 kg of hop_northern brewer\n",
      "0.0028988600242882967 kg of hop_saaz\n",
      "0.35830119252204895 kg of hop_simcoe\n",
      "0.00014166184701025486 kg of hop_tettnanger\n",
      "0.0010747439227998257 kg of hop_willamette\n",
      "0.0007605288992635906 kg of yeast_american ale\n",
      "0.0001457096659578383 kg of yeast_london esb ale\n",
      "0.00039416152867488563 kg of yeast_safale s-04\n",
      "0.45799535512924194 kg of yeast_safale us-05\n",
      "0.3377273678779602 kg of misc_irish moss\n",
      "0.00011748338874895126 kg of misc_lavender\n",
      "0.0004938271595165133 kg of misc_whirlfloc tablet\n",
      "And boil for 60.99998474121094 minutes.\n"
     ]
    }
   ],
   "source": [
    "for ing in ing_ids:\n",
    "    if ing != 687:\n",
    "        print(f\"{random_beer[ing]} kg of {inv_vocab[ing]}\")\n",
    "    else:\n",
    "        print(f\"And boil for {random_beer[ing]} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp, our first brew looks to be a crime against brewmanity. Essentially, the average amount of every ingredient across all beers? A nice baseline to improve on, though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas from Ethan\n",
    "\n",
    "\n",
    "## Model Ideas\n",
    "\n",
    "* Look at adding regularization in between layers\n",
    "* Look at VAE\n",
    "* Look at GAN\n",
    "  * This is most interesting\n",
    "  * Its explicit job is creating recipes that look real.\n",
    "  * Discriminator needs to have some function that estimates \"quality\" of what is produced (decoded space).\n",
    "\n",
    "Combining the properties of a VAE (where you can interpolate) and a GAN (where you can estimate \"quality\" of a recipe) would be perfect.\n",
    "\n",
    "Main issue is that reconstruction error doesn't give you the properties you want. Look for other learning signals - what other aspect of a recipe is meaningful? Could we use the beer style as a label?\n",
    "\n",
    "\n",
    "## Encoding Ideas\n",
    "\n",
    "* Can you encode hops into all possible combinations present in the dataset (not all combinations period)? Rob thinks this is reasonable\n",
    "* Can we make ingredient \"categories\"? e.g. caramel with varying lovabond measurement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-rory/repos/beer.ai/env/lib/python3.8/site-packages/torch/nn/functional.py:2398: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.123690\n",
      "epoch [2/100], loss:0.122374\n",
      "epoch [3/100], loss:0.120745\n",
      "epoch [4/100], loss:0.118553\n",
      "epoch [5/100], loss:0.115636\n",
      "epoch [6/100], loss:0.111839\n",
      "epoch [7/100], loss:0.107007\n",
      "epoch [8/100], loss:0.101006\n",
      "epoch [9/100], loss:0.093692\n",
      "epoch [10/100], loss:0.084982\n",
      "epoch [11/100], loss:0.074893\n",
      "epoch [12/100], loss:0.063603\n",
      "epoch [13/100], loss:0.051501\n",
      "epoch [14/100], loss:0.039232\n",
      "epoch [15/100], loss:0.027659\n",
      "epoch [16/100], loss:0.017709\n",
      "epoch [17/100], loss:0.010080\n",
      "epoch [18/100], loss:0.004953\n",
      "epoch [19/100], loss:0.001950\n",
      "epoch [20/100], loss:0.000403\n",
      "epoch [21/100], loss:-0.000313\n",
      "epoch [22/100], loss:-0.000619\n",
      "epoch [23/100], loss:-0.000742\n",
      "epoch [24/100], loss:-0.000790\n",
      "epoch [25/100], loss:-0.000807\n",
      "epoch [26/100], loss:-0.000814\n",
      "epoch [27/100], loss:-0.000816\n",
      "epoch [28/100], loss:-0.000816\n",
      "epoch [29/100], loss:-0.000817\n",
      "epoch [30/100], loss:-0.000816\n",
      "epoch [31/100], loss:-0.000816\n",
      "epoch [32/100], loss:-0.000816\n",
      "epoch [33/100], loss:-0.000817\n",
      "epoch [34/100], loss:-0.000817\n",
      "epoch [35/100], loss:-0.000817\n",
      "epoch [36/100], loss:-0.000817\n",
      "epoch [37/100], loss:-0.000818\n",
      "epoch [38/100], loss:-0.000818\n",
      "epoch [39/100], loss:-0.000819\n",
      "epoch [40/100], loss:-0.000821\n",
      "epoch [41/100], loss:-0.000823\n",
      "epoch [42/100], loss:-0.000825\n",
      "epoch [43/100], loss:-0.000829\n",
      "epoch [44/100], loss:-0.000835\n",
      "epoch [45/100], loss:-0.000841\n",
      "epoch [46/100], loss:-0.000845\n",
      "epoch [47/100], loss:-0.000841\n",
      "epoch [48/100], loss:-0.000835\n",
      "epoch [49/100], loss:-0.000836\n",
      "epoch [50/100], loss:-0.000841\n",
      "epoch [51/100], loss:-0.000845\n",
      "epoch [52/100], loss:-0.000845\n",
      "epoch [53/100], loss:-0.000843\n",
      "epoch [54/100], loss:-0.000841\n",
      "epoch [55/100], loss:-0.000840\n",
      "epoch [56/100], loss:-0.000840\n",
      "epoch [57/100], loss:-0.000841\n",
      "epoch [58/100], loss:-0.000842\n",
      "epoch [59/100], loss:-0.000843\n",
      "epoch [60/100], loss:-0.000844\n",
      "epoch [61/100], loss:-0.000845\n",
      "epoch [62/100], loss:-0.000844\n",
      "epoch [63/100], loss:-0.000843\n",
      "epoch [64/100], loss:-0.000843\n",
      "epoch [65/100], loss:-0.000843\n",
      "epoch [66/100], loss:-0.000844\n",
      "epoch [67/100], loss:-0.000845\n",
      "epoch [68/100], loss:-0.000845\n",
      "epoch [69/100], loss:-0.000844\n",
      "epoch [70/100], loss:-0.000844\n",
      "epoch [71/100], loss:-0.000844\n",
      "epoch [72/100], loss:-0.000844\n",
      "epoch [73/100], loss:-0.000844\n",
      "epoch [74/100], loss:-0.000845\n",
      "epoch [75/100], loss:-0.000845\n",
      "epoch [76/100], loss:-0.000845\n",
      "epoch [77/100], loss:-0.000844\n",
      "epoch [78/100], loss:-0.000844\n",
      "epoch [79/100], loss:-0.000844\n",
      "epoch [80/100], loss:-0.000844\n",
      "epoch [81/100], loss:-0.000845\n",
      "epoch [82/100], loss:-0.000845\n",
      "epoch [83/100], loss:-0.000845\n",
      "epoch [84/100], loss:-0.000845\n",
      "epoch [85/100], loss:-0.000844\n",
      "epoch [86/100], loss:-0.000844\n",
      "epoch [87/100], loss:-0.000845\n",
      "epoch [88/100], loss:-0.000845\n",
      "epoch [89/100], loss:-0.000845\n",
      "epoch [90/100], loss:-0.000845\n",
      "epoch [91/100], loss:-0.000845\n",
      "epoch [92/100], loss:-0.000845\n",
      "epoch [93/100], loss:-0.000845\n",
      "epoch [94/100], loss:-0.000845\n",
      "epoch [95/100], loss:-0.000845\n",
      "epoch [96/100], loss:-0.000845\n",
      "epoch [97/100], loss:-0.000845\n",
      "epoch [98/100], loss:-0.000845\n",
      "epoch [99/100], loss:-0.000845\n",
      "epoch [100/100], loss:-0.000845\n"
     ]
    }
   ],
   "source": [
    "input_dims = len(recipes.columns)\n",
    "# first guess\n",
    "compress_dims = 50\n",
    "# factor to reduce by each layer\n",
    "factor_per_layer = 2\n",
    "beer_ae = AutoEncoder(input_dims, compress_dims, factor_per_layer)\n",
    "\n",
    "# training params\n",
    "batch_size = 50000\n",
    "num_epochs=100\n",
    "learning_rate=1e-3\n",
    "# weight of KL loss term\n",
    "beta = 0.5\n",
    "train(beer_ae, recipes_scaled, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer = generate_random_beer(beer_ae)\n",
    "ing_ids = np.where(random_beer > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0015515829436481 kg of ferm_2-row\n",
      "0.00012997942394576967 kg of ferm_6-row\n",
      "0.0004142947727814317 kg of ferm_abbey malt\n",
      "0.00033476034877821803 kg of ferm_acidulated malt\n",
      "0.0006134641589596868 kg of ferm_amber malt\n",
      "0.0001865856465883553 kg of ferm_aromatic barley malt\n",
      "0.000759866030421108 kg of ferm_biscuit® md™\n",
      "0.0002605172630865127 kg of ferm_black malt\n",
      "0.0011127550387755036 kg of ferm_brown malt\n",
      "0.00027970969676971436 kg of ferm_brown sugar\n",
      "0.00015848602924961597 kg of ferm_brown sugar, dark\n",
      "0.00024047333863563836 kg of ferm_brown sugar, light\n",
      "0.00011076791997766122 kg of ferm_candi sugar, amber\n",
      "0.000290135940304026 kg of ferm_candi sugar, clear\n",
      "0.00035931949969381094 kg of ferm_cara malt\n",
      "0.00014394363097380847 kg of ferm_caraaroma\n",
      "0.0001915988977998495 kg of ferm_carafa i\n",
      "0.00019545425311662257 kg of ferm_carafa ii\n",
      "0.0002506965829525143 kg of ferm_carafa iii\n",
      "0.00017981529526878148 kg of ferm_carafa special iii\n",
      "0.0001679700071690604 kg of ferm_carafoam\n",
      "0.00017033125914167613 kg of ferm_carahell®\n",
      "0.00017318590835202485 kg of ferm_caramel malt 10l\n",
      "0.006391276139765978 kg of ferm_caramel malt 120l\n",
      "0.000244075883529149 kg of ferm_caramel malt 20l\n",
      "0.0002606689231470227 kg of ferm_caramel malt 40l\n",
      "0.0002572116209194064 kg of ferm_caramel malt 80l\n",
      "0.0013461242197081447 kg of ferm_caramel/crystal 60l\n",
      "0.00015004117449279875 kg of ferm_caramel/crystal 90l\n",
      "0.0004105334519408643 kg of ferm_caramunich i\n",
      "0.0003893895773217082 kg of ferm_caramunich ii\n",
      "0.0016403849003836513 kg of ferm_carapils® malt\n",
      "0.0018851890927180648 kg of ferm_carared\n",
      "0.001068487879820168 kg of ferm_chocolate\n",
      "0.0002658628800418228 kg of ferm_chocolate malt\n",
      "0.00022627598082181066 kg of ferm_chocolate wheat\n",
      "0.00028976984322071075 kg of ferm_coffee malt\n",
      "0.0001435567974112928 kg of ferm_corn sugar (dextrose)\n",
      "0.0005842946702614427 kg of ferm_crystal malt\n",
      "0.00024207546084653586 kg of ferm_crystal rye\n",
      "0.00013005861546844244 kg of ferm_dark dry malt extract\n",
      "0.00023272988619282842 kg of ferm_dry malt extract - amber\n",
      "0.003980239853262901 kg of ferm_dry malt extract - light\n",
      "0.0006760768592357635 kg of ferm_dry malt extract - pilsen\n",
      "0.00033455013181082904 kg of ferm_dry malt extract - wheat\n",
      "0.0011897367658093572 kg of ferm_flaked oats\n",
      "0.0006030636723153293 kg of ferm_flaked wheat\n",
      "0.00015161570627242327 kg of ferm_golden naked oats\n",
      "0.00010548321006353945 kg of ferm_golden promise\n",
      "0.0011428515426814556 kg of ferm_honey\n",
      "0.0017476835055276752 kg of ferm_honey malt\n",
      "0.0008156612166203558 kg of ferm_liquid malt extract - amber\n",
      "0.00127044424880296 kg of ferm_liquid malt extract - dark\n",
      "0.00042920521809719503 kg of ferm_liquid malt extract - light\n",
      "0.00010924998787231743 kg of ferm_liquid malt extract - pilsen\n",
      "0.00010316626139683649 kg of ferm_liquid malt extract - wheat\n",
      "0.0009239212376996875 kg of ferm_maltodextrine\n",
      "0.005540367681533098 kg of ferm_maris otter pale\n",
      "0.00048764230450615287 kg of ferm_melanoidin\n",
      "0.0001393943530274555 kg of ferm_molasses\n",
      "0.00029301008908078074 kg of ferm_munich malt\n",
      "0.00011960463598370552 kg of ferm_munich malt - 20l\n",
      "0.0001163261040346697 kg of ferm_munich malt 10l\n",
      "0.0003705420240294188 kg of ferm_munich type i\n",
      "0.00010952234879368916 kg of ferm_munich type ii\n",
      "0.0008032653713598847 kg of ferm_pale malt\n",
      "0.0004968408029526472 kg of ferm_pilsen malt\n",
      "0.0001654771767789498 kg of ferm_rice hulls\n",
      "0.005348522681742907 kg of ferm_roasted barley\n",
      "0.00038076064083725214 kg of ferm_rye\n",
      "0.00017411098815500736 kg of ferm_rye malt\n",
      "0.0010680363047868013 kg of ferm_special b\n",
      "0.0002792475279420614 kg of ferm_sugar, table (sucrose)\n",
      "0.000684201717376709 kg of ferm_vienna\n",
      "0.00033571082167327404 kg of ferm_white wheat\n",
      "0.00011733994324458763 kg of hop_admiral\n",
      "0.00014222080062609166 kg of hop_amarillo\n",
      "0.00022169112344272435 kg of hop_aurora\n",
      "0.0016245825681835413 kg of hop_azacca\n",
      "0.0015211172867566347 kg of hop_bramling cross\n",
      "0.0004829429090023041 kg of hop_bravo\n",
      "0.0004242294526193291 kg of hop_buzz bullets\n",
      "0.00038899050559848547 kg of hop_calypso\n",
      "0.004854422062635422 kg of hop_cascade\n",
      "0.00033064885064959526 kg of hop_centennial\n",
      "0.00019945028179790825 kg of hop_challenger\n",
      "0.004269065335392952 kg of hop_chinook\n",
      "0.010435624048113823 kg of hop_citra\n",
      "0.00029259661096148193 kg of hop_cluster\n",
      "0.00014280114555731416 kg of hop_delta\n",
      "0.0006298702210187912 kg of hop_east kent golding\n",
      "0.0006974184070713818 kg of hop_el dorado\n",
      "0.00012790216715075076 kg of hop_ella (stella)\n",
      "0.0001240163983311504 kg of hop_equinox\n",
      "0.0007639320101588964 kg of hop_falconer's flight\n",
      "0.00030896157841198146 kg of hop_fuggle\n",
      "0.0003869757638312876 kg of hop_galaxy\n",
      "0.00012721707753371447 kg of hop_galena\n",
      "0.00024963548639789224 kg of hop_green bullet\n",
      "0.0009556855075061321 kg of hop_hallertau\n",
      "0.0003777189995162189 kg of hop_hallertauer mittelfrüh\n",
      "0.0001396790030412376 kg of hop_herkules\n",
      "0.0001832979905884713 kg of hop_huell melon\n",
      "0.0005804285756312311 kg of hop_jarrylo\n",
      "0.00018674434977583587 kg of hop_junga\n",
      "0.00024077085254248232 kg of hop_kohatu\n",
      "0.0011560129933059216 kg of hop_liberty\n",
      "0.0009682865929789841 kg of hop_magnum\n",
      "0.00044605828588828444 kg of hop_mosaic™\n",
      "0.00015240133507177234 kg of hop_motueka\n",
      "0.00039791129529476166 kg of hop_mount hood\n",
      "0.00016910153499338776 kg of hop_nelson sauvin\n",
      "0.0008319222833961248 kg of hop_northern brewer\n",
      "0.00017322375788353384 kg of hop_nugget\n",
      "0.000156274953042157 kg of hop_pacific gem\n",
      "0.00024265199317596853 kg of hop_pacific jade\n",
      "0.0015737423673272133 kg of hop_pacifica\n",
      "0.00014720363833475858 kg of hop_perle\n",
      "0.00010965835099341348 kg of hop_pilgrim\n",
      "0.0018911303486675024 kg of hop_saaz\n",
      "0.0002203042822657153 kg of hop_simcoe\n",
      "0.00013544291141442955 kg of hop_smaragd\n",
      "0.0009656738257035613 kg of hop_sorachi ace\n",
      "0.00036897952668368816 kg of hop_southern cross\n",
      "0.00015410453488584608 kg of hop_sterling\n",
      "0.0002569815260358155 kg of hop_styrian bobek\n",
      "0.0006282216054387391 kg of hop_styrian golding\n",
      "0.00029436268960125744 kg of hop_summit\n",
      "0.00014719270984642208 kg of hop_target\n",
      "0.0003855990362353623 kg of hop_tettnanger\n",
      "0.00015357177471742034 kg of hop_tnt\n",
      "0.0003441606822889298 kg of hop_tradition\n",
      "0.00010735017713159323 kg of hop_warrior\n",
      "0.0003390526107978076 kg of hop_willamette\n",
      "0.00018955906853079796 kg of hop_ahtanum_dry\n",
      "0.0001706140610622242 kg of hop_amarillo_dry\n",
      "0.0002048912429017946 kg of hop_bramling cross_dry\n",
      "0.0006864018505439162 kg of hop_cascade_dry\n",
      "0.0007790870149619877 kg of hop_centennial_dry\n",
      "0.00010271780047332868 kg of hop_chinook_dry\n",
      "0.002689975779503584 kg of hop_citra_dry\n",
      "0.00012765004066750407 kg of hop_east kent golding_dry\n",
      "0.0001526805863250047 kg of hop_first gold_dry\n",
      "0.0002338765189051628 kg of hop_galaxy_dry\n",
      "0.00013147683057468385 kg of hop_glacier_dry\n",
      "0.00022665620781481266 kg of hop_kohatu_dry\n",
      "0.00022287214233074337 kg of hop_motueka_dry\n",
      "0.00025912499404512346 kg of hop_smaragd_dry\n",
      "0.00028902399935759604 kg of hop_sorachi ace_dry\n",
      "0.00016714866796974093 kg of hop_styrian bobek_dry\n",
      "0.00010944072710117325 kg of yeast_- pac man (1764)\n",
      "0.0002874456695280969 kg of yeast_american ale\n",
      "0.0005974408122710884 kg of yeast_american ale ii\n",
      "0.0006966395885683596 kg of yeast_american wheat\n",
      "0.0010013208957388997 kg of yeast_belgian lambic blend\n",
      "0.0002432018518447876 kg of yeast_belgian wit ale\n",
      "0.0006581062916666269 kg of yeast_belle saison\n",
      "0.00019995696493424475 kg of yeast_california ale\n",
      "0.000401358469389379 kg of yeast_danish lager\n",
      "0.0001340925373369828 kg of yeast_denny's favorite 50\n",
      "0.0013392684049904346 kg of yeast_french saison\n",
      "0.00019314074597787112 kg of yeast_german wheat\n",
      "0.00022515069576911628 kg of yeast_lactobacillus\n",
      "0.0007683475851081312 kg of yeast_london esb ale\n",
      "0.00017443657270632684 kg of yeast_newcastle dark ale\n",
      "0.00025995515170507133 kg of yeast_premium gold\n",
      "0.0041070133447647095 kg of yeast_safale s-04\n",
      "0.0019107010448351502 kg of yeast_safale us-05\n",
      "0.0004262951260898262 kg of yeast_safbrew s-33\n",
      "0.0001538364595035091 kg of yeast_safbrew t-58\n",
      "0.00021413445938378572 kg of yeast_safbrew wb-06\n",
      "0.000558667175937444 kg of yeast_saflager s-23\n",
      "0.00016516105097252876 kg of yeast_trappist high gravity\n",
      "0.001094713225029409 kg of yeast_u.s. west coast\n",
      "0.0001785404747352004 kg of yeast_west yorkshire (1469)\n",
      "0.00023184689052868634 kg of yeast_whitbread ale\n",
      "0.00029325459036044776 kg of yeast_windsor\n",
      "0.00012027702177874744 kg of yeast_workhorse\n",
      "0.00024520838633179665 kg of misc_allspice\n",
      "0.00011485334835015237 kg of misc_allspice powder\n",
      "0.0006271708407439291 kg of misc_bitter orange peel\n",
      "0.0001707926858216524 kg of misc_black pepper\n",
      "0.00021366996224969625 kg of misc_blackberries\n",
      "0.00017590972129255533 kg of misc_blueberries\n",
      "0.00010485502571100369 kg of misc_bourbon\n",
      "0.00029233546229079366 kg of misc_brown sugar\n",
      "0.0017283298075199127 kg of misc_cherries\n",
      "0.0001233314978890121 kg of misc_chocolate\n",
      "0.0006603171932511032 kg of misc_cinnamon sticks\n",
      "0.005215634126216173 kg of misc_cocoa nibs\n",
      "0.00012474128743633628 kg of misc_cocoa powder\n",
      "0.005509268492460251 kg of misc_coffee\n",
      "0.0001799716701498255 kg of misc_coffee grounds\n",
      "0.000183107316843234 kg of misc_coriander seed\n",
      "0.0001440834312234074 kg of misc_corn sugar\n",
      "0.0002130003267666325 kg of misc_dextrose\n",
      "0.000308839458739385 kg of misc_espresso\n",
      "0.0004705847823061049 kg of misc_ginger root\n",
      "0.0002882677363231778 kg of misc_gypsum\n",
      "0.0020003116223961115 kg of misc_heather tips\n",
      "0.001147037954069674 kg of misc_honey\n",
      "0.00010508589912205935 kg of misc_irish moss\n",
      "0.0001366050128126517 kg of misc_juniper berries\n",
      "0.00020268248044885695 kg of misc_lactic acid\n",
      "0.00010723526065703481 kg of misc_lactose\n",
      "0.00013688708713743836 kg of misc_lavender\n",
      "0.0005438164807856083 kg of misc_lemon juice\n",
      "0.00038035496254451573 kg of misc_lemon zest\n",
      "0.0005360546638257802 kg of misc_lemongrass\n",
      "0.00033115892438218 kg of misc_mango\n",
      "0.0001159749153885059 kg of misc_nutmeg\n",
      "0.00017331891285721213 kg of misc_oak chips\n",
      "0.0003229274007026106 kg of misc_orange juice\n",
      "0.0002295677550137043 kg of misc_orange zest\n",
      "0.00022805455955676734 kg of misc_peaches\n",
      "0.001522377715446055 kg of misc_pumpkin\n",
      "0.0003274315968155861 kg of misc_pumpkin pie spice\n",
      "0.00041858895565383136 kg of misc_raisins\n",
      "0.0007382862386293709 kg of misc_raspberry\n",
      "0.00015303776308428496 kg of misc_salt\n",
      "0.00016438150487374514 kg of misc_sinamar\n",
      "0.00013397879956755787 kg of misc_star anise\n",
      "0.0005441185785457492 kg of misc_strawberries\n",
      "0.00016422801127191633 kg of misc_sweet orange peel\n",
      "0.00023378174228128046 kg of misc_toasted coconut\n",
      "0.00028897137963213027 kg of misc_vanilla beans\n",
      "0.0007171171018853784 kg of misc_whirlfloc tablet\n",
      "0.00037992114084772766 kg of misc_yeast nutrient\n",
      "And boil for 101.28536987304688 minutes.\n"
     ]
    }
   ],
   "source": [
    "for ing in ing_ids:\n",
    "    if ing != 687:\n",
    "        print(f\"{random_beer[ing]} kg of {inv_vocab[ing]}\")\n",
    "    else:\n",
    "        print(f\"And boil for {random_beer[ing]} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Vector: name\n",
      "0.0           1.553145\n",
      "34.0          1.553145\n",
      "49.0          1.553145\n",
      "74.0          1.553145\n",
      "85.0          1.553145\n",
      "187.0         3.106290\n",
      "252.0         1.553145\n",
      "576.0         1.553145\n",
      "613.0         1.553145\n",
      "623.0         1.553145\n",
      "639.0         3.106290\n",
      "645.0         1.553145\n",
      "658.0         1.553145\n",
      "664.0         3.106290\n",
      "665.0         1.553145\n",
      "boil_time    60.000000\n",
      "Name: 7, dtype: float64\n",
      "1.5531449289298356 kg of ferm_2-row\n",
      "1.5531449289298356 kg of ferm_caramel malt 40l\n",
      "1.5531449289298356 kg of ferm_chocolate malt\n",
      "1.5531449289298356 kg of ferm_honey malt\n",
      "1.5531449289298356 kg of ferm_maris otter pale\n",
      "3.1062898578596707 kg of hop_hallertau\n",
      "1.5531449289298356 kg of hop_saaz\n",
      "1.5531449289298356 kg of yeast_safale us-05\n",
      "1.5531449289298356 kg of misc_brown sugar\n",
      "1.5531449289298356 kg of misc_cinnamon\n",
      "3.1062898578596707 kg of misc_grains of paradise\n",
      "1.5531449289298356 kg of misc_irish moss\n",
      "1.5531449289298356 kg of misc_nutmeg\n",
      "3.1062898578596707 kg of misc_pumpkin\n",
      "1.5531449289298356 kg of misc_pumpkin pie spice\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'boil_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-5279b0b02923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Recipe Vector: {rec}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0ming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ming\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m687\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{rec.loc[ing]} kg of {inv_vocab[ing]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'boil_time'"
     ]
    }
   ],
   "source": [
    "rec = recipes.loc[7, recipes.loc[7] != 0]\n",
    "print(f\"Recipe Vector: {rec}\")\n",
    "for ing in rec.index:\n",
    "    ing = int(ing)\n",
    "    if ing != 687:\n",
    "        print(f\"{rec.loc[ing]} kg of {inv_vocab[ing]}\")\n",
    "    else:\n",
    "        print(f\"And boil for {rec.loc[ing]} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After trying for a few times with various betas (KL Divergence parameter), it doesn't look super promising. We can mess around with dimensionality I suppose, but it seems like it might be better to try a VAE. Let's do that next time!\n",
    "\n",
    "Examples:\n",
    "* https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "* https://github.com/pytorch/examples/blob/master/vae/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Ahmed\n",
    "\n",
    "* Consider using TF-IDF vectorizer (or maybe TF-DF)\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beer.ai",
   "language": "python",
   "name": "beer.ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
