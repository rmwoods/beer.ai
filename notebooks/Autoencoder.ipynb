{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "from beerai.config import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors():\n",
    "    vec_file = os.path.join(DATA_DIR, \"processed/recipe_vecs.h5\")\n",
    "    with pd.HDFStore(vec_file, \"r\") as store:\n",
    "        vectors = store.get(\"/vecs\")\n",
    "    return vectors\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_file = os.path.join(DATA_DIR, \"processed/vocab.pickle\")\n",
    "    with open(vocab_file, \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    inv_vocab = {v: k for k,v in vocab.items()}\n",
    "    return vocab, inv_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = load_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>...</th>\n",
       "      <th>783.0</th>\n",
       "      <th>784.0</th>\n",
       "      <th>785.0</th>\n",
       "      <th>786.0</th>\n",
       "      <th>787.0</th>\n",
       "      <th>788.0</th>\n",
       "      <th>789.0</th>\n",
       "      <th>790.0</th>\n",
       "      <th>791.0</th>\n",
       "      <th>boil_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>0.103101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 793 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "name            0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  ...  783.0  \\\n",
       "recipe_id                                                         ...          \n",
       "0.0        0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "1.0        0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "5.0        0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "7.0        0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "8.0        0.103101  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
       "\n",
       "name       784.0  785.0  786.0     787.0  788.0  789.0  790.0  791.0  \\\n",
       "recipe_id                                                              \n",
       "0.0          0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "1.0          0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "5.0          0.0    0.0    0.0  0.024016    0.0    0.0    0.0    0.0   \n",
       "7.0          0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "8.0          0.0    0.0    0.0  0.000000    0.0    0.0    0.0    0.0   \n",
       "\n",
       "name       boil_time  \n",
       "recipe_id             \n",
       "0.0             60.0  \n",
       "1.0             60.0  \n",
       "5.0             90.0  \n",
       "7.0             60.0  \n",
       "8.0             60.0  \n",
       "\n",
       "[5 rows x 793 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes[\"boil_time\"] = recipes.boil_time.clip(upper=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "recipes_scaled = scaler.fit_transform(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on sparse autoencoders\n",
    "\n",
    "- [Discussion on quora](https://www.quora.com/When-training-an-autoencoder-on-very-sparse-data-how-do-you-force-the-decoder-to-reconstruct-mostly-zeros-rather-than-always-just-reconstructing-the-average)\n",
    "  - > So, if you're including a bias term in your autoencoders, I recommend removing the bias and attempting training again.\n",
    "  - > I encountered this problem recently. I found that, similar to what Eric described, it is a problem with SGD getting stuck in bad local optima. A couple things helped: (1) using conjugate gradients or AdaGrad, either of which will find the path to the true minimum without getting stuck as much as plain SGD will; (2) using a combined cross-entropy & mean-squared-error loss function (assuming that you can model your data as binary vectors or as probability distributions) pulls things -- ever so slightly -- in the right directions better than either alone would.\n",
    "  - > I decided to progressively lower the learning rate and then I got good results. So that's my tip, lower your learning rate until you get better results. (For example, I'm using now: 0.0000005 as the initial rate.) And don’t forget to normalize your data!\n",
    "  - > You shouldn't need to do anything special for this. Standard good practices for initialization and training should take care of it.\n",
    "  - > We were able to reproduce the original image (not get the average) by using AdamOptimizer and lowering the learning rate.\n",
    "  - [Notes](http://web.stanford.edu/class/cs294a/sae/sparseAutoencoderNotes.pdf) from Stanford CS294a (Andrew Ng)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        compression_dim,\n",
    "        factor_per_layer,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.compression_dim = compression_dim\n",
    "\n",
    "        comp_layers, decomp_layers = self.gen_layers_by_factor(\n",
    "            input_dim, compression_dim, factor_per_layer\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.Sequential(*comp_layers)\n",
    "        self.decoder = nn.Sequential(*decomp_layers)\n",
    " \n",
    "    def gen_layers_by_factor(self, input_dim, compression_dim, factor_per_layer):\n",
    "        \n",
    "        cur_dim = input_dim\n",
    "        compress_layers = []\n",
    "        decompress_layers = []\n",
    "        n_iters = math.ceil(math.log(input_dim / compression_dim, factor_per_layer))\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            new_dim = max(cur_dim // factor_per_layer, compression_dim)\n",
    "            compress_layers.extend([nn.Linear(cur_dim, new_dim, bias=False), nn.ReLU(True)])\n",
    "            decompress_layers.extend([nn.ReLU(True), nn.Linear(new_dim, cur_dim, bias=False)])\n",
    "            cur_dim = new_dim\n",
    "        decompress_layers = decompress_layers[::-1]\n",
    "        # Replace final layer with sigmoid/tanh. Should match the input scaling range \n",
    "        compress_layers[-1] = nn.Sigmoid()\n",
    "        decompress_layers[-1] = nn.Sigmoid()\n",
    "\n",
    "        return compress_layers, decompress_layers\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoded = self.encoder(X)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self.encoder(X)\n",
    "        return pd.DataFrame(X.detach().numpy())\n",
    "    \n",
    "    def decode(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        X = self.decoder(X)\n",
    "        return pd.DataFrame(X.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, batch_size, num_epochs, learning_rate=1e-3, beta=1):\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    kldiv = nn.KLDivLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-5\n",
    "    )\n",
    "\n",
    "    #features = torch.tensor(X, dtype=torch.float)\n",
    "    data = torch.tensor(X, dtype=torch.float)\n",
    "    \n",
    "    #train = data_utils.TensorDataset(features)\n",
    "    \n",
    "    # Shuffle used to ensure randomized selection\n",
    "    #train_loader = data_utils.DataLoader(\n",
    "    #    train, batch_size=batch_size, shuffle=True\n",
    "    #)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        #for i, batch in enumerate(train_loader):\n",
    "            # ===================forward=====================\n",
    "            #data = batch[0]\n",
    "        decoded = model.forward(data)\n",
    "        length = decoded.shape[0]\n",
    "\n",
    "        if length < batch_size:\n",
    "            continue\n",
    "        mse_loss = mse(decoded, data)\n",
    "        kld_loss = kldiv(decoded, data)\n",
    "        loss = mse_loss + beta * kld_loss\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data)\n",
    "\n",
    "        print(f\"epoch [{epoch + 1}/{num_epochs}], loss:{sum(losses)/len(losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = len(recipes.columns)\n",
    "# first guess\n",
    "compress_dims = 50\n",
    "# factor to reduce by each layer\n",
    "factor_per_layer = 2\n",
    "beer_ae = AutoEncoder(input_dims, compress_dims, factor_per_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rory/repos/beer.ai/env/lib/python3.6/site-packages/torch/nn/functional.py:2352: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\"reduction: 'mean' divides the total loss by both the batch size and the support size.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/100], loss:0.236223\n",
      "epoch [2/100], loss:0.234687\n",
      "epoch [3/100], loss:0.231933\n",
      "epoch [4/100], loss:0.227674\n",
      "epoch [5/100], loss:0.222122\n",
      "epoch [6/100], loss:0.213805\n",
      "epoch [7/100], loss:0.202950\n",
      "epoch [8/100], loss:0.188608\n",
      "epoch [9/100], loss:0.170590\n",
      "epoch [10/100], loss:0.148501\n",
      "epoch [11/100], loss:0.122638\n",
      "epoch [12/100], loss:0.094110\n",
      "epoch [13/100], loss:0.064928\n",
      "epoch [14/100], loss:0.038338\n",
      "epoch [15/100], loss:0.017751\n",
      "epoch [16/100], loss:0.004928\n",
      "epoch [17/100], loss:-0.001124\n",
      "epoch [18/100], loss:-0.003183\n",
      "epoch [19/100], loss:-0.003633\n",
      "epoch [20/100], loss:-0.003651\n",
      "epoch [21/100], loss:-0.003597\n",
      "epoch [22/100], loss:-0.003576\n",
      "epoch [23/100], loss:-0.003594\n",
      "epoch [24/100], loss:-0.003634\n",
      "epoch [25/100], loss:-0.003682\n",
      "epoch [26/100], loss:-0.003758\n",
      "epoch [27/100], loss:-0.003871\n",
      "epoch [28/100], loss:-0.003985\n",
      "epoch [29/100], loss:-0.004042\n",
      "epoch [30/100], loss:-0.004015\n",
      "epoch [31/100], loss:-0.004050\n",
      "epoch [32/100], loss:-0.004061\n",
      "epoch [33/100], loss:-0.004116\n",
      "epoch [34/100], loss:-0.004165\n",
      "epoch [35/100], loss:-0.004160\n",
      "epoch [36/100], loss:-0.004137\n",
      "epoch [37/100], loss:-0.004135\n",
      "epoch [38/100], loss:-0.004148\n",
      "epoch [39/100], loss:-0.004156\n",
      "epoch [40/100], loss:-0.004232\n",
      "epoch [41/100], loss:-0.004277\n",
      "epoch [42/100], loss:-0.004257\n",
      "epoch [43/100], loss:-0.004229\n",
      "epoch [44/100], loss:-0.004239\n",
      "epoch [45/100], loss:-0.004285\n",
      "epoch [46/100], loss:-0.004338\n",
      "epoch [47/100], loss:-0.004333\n",
      "epoch [48/100], loss:-0.004312\n",
      "epoch [49/100], loss:-0.004329\n",
      "epoch [50/100], loss:-0.004346\n",
      "epoch [51/100], loss:-0.004366\n",
      "epoch [52/100], loss:-0.004398\n",
      "epoch [53/100], loss:-0.004420\n",
      "epoch [54/100], loss:-0.004365\n",
      "epoch [55/100], loss:-0.004402\n",
      "epoch [56/100], loss:-0.004434\n",
      "epoch [57/100], loss:-0.004428\n",
      "epoch [58/100], loss:-0.004414\n",
      "epoch [59/100], loss:-0.004409\n",
      "epoch [60/100], loss:-0.004412\n",
      "epoch [61/100], loss:-0.004418\n",
      "epoch [62/100], loss:-0.004426\n",
      "epoch [63/100], loss:-0.004435\n",
      "epoch [64/100], loss:-0.004442\n",
      "epoch [65/100], loss:-0.004440\n",
      "epoch [66/100], loss:-0.004435\n",
      "epoch [67/100], loss:-0.004437\n",
      "epoch [68/100], loss:-0.004443\n",
      "epoch [69/100], loss:-0.004447\n",
      "epoch [70/100], loss:-0.004445\n",
      "epoch [71/100], loss:-0.004444\n",
      "epoch [72/100], loss:-0.004443\n",
      "epoch [73/100], loss:-0.004445\n",
      "epoch [74/100], loss:-0.004447\n",
      "epoch [75/100], loss:-0.004450\n",
      "epoch [76/100], loss:-0.004451\n",
      "epoch [77/100], loss:-0.004451\n",
      "epoch [78/100], loss:-0.004449\n",
      "epoch [79/100], loss:-0.004451\n",
      "epoch [80/100], loss:-0.004452\n",
      "epoch [81/100], loss:-0.004454\n",
      "epoch [82/100], loss:-0.004455\n",
      "epoch [83/100], loss:-0.004456\n",
      "epoch [84/100], loss:-0.004459\n",
      "epoch [85/100], loss:-0.004468\n",
      "epoch [86/100], loss:-0.004488\n",
      "epoch [87/100], loss:-0.004511\n",
      "epoch [88/100], loss:-0.004452\n",
      "epoch [89/100], loss:-0.004496\n",
      "epoch [90/100], loss:-0.004509\n",
      "epoch [91/100], loss:-0.004498\n",
      "epoch [92/100], loss:-0.004489\n",
      "epoch [93/100], loss:-0.004484\n",
      "epoch [94/100], loss:-0.004481\n",
      "epoch [95/100], loss:-0.004483\n",
      "epoch [96/100], loss:-0.004487\n",
      "epoch [97/100], loss:-0.004494\n",
      "epoch [98/100], loss:-0.004503\n",
      "epoch [99/100], loss:-0.004510\n",
      "epoch [100/100], loss:-0.004508\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10000\n",
    "num_epochs=100\n",
    "learning_rate=1e-3\n",
    "# weight of KL loss term\n",
    "beta = 0.1\n",
    "train(beer_ae, recipes_scaled, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)\n",
    "#train(beer_ae, recipes.values, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = beer_ae.decode(beer_ae.encode(recipes.values).values)\n",
    "decoded.index = recipes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded[decoded < 1e-6] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "      <th>785</th>\n",
       "      <th>786</th>\n",
       "      <th>787</th>\n",
       "      <th>788</th>\n",
       "      <th>789</th>\n",
       "      <th>790</th>\n",
       "      <th>791</th>\n",
       "      <th>792</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recipe_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404601.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404604.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404606.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404622.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404634.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.324089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.554019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171699 rows × 793 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0    1    2    3         4    5    6    7    8    9    ...  783  \\\n",
       "recipe_id                                                         ...        \n",
       "0.0        0.0  0.0  0.0  0.0  0.324063  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1.0        0.0  0.0  0.0  0.0  0.324081  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "5.0        0.0  0.0  0.0  0.0  0.321901  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "7.0        0.0  0.0  0.0  0.0  0.324039  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "8.0        0.0  0.0  0.0  0.0  0.324050  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "...        ...  ...  ...  ...       ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "404601.0   0.0  0.0  0.0  0.0  0.324081  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404604.0   0.0  0.0  0.0  0.0  0.324083  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404606.0   0.0  0.0  0.0  0.0  0.324090  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404622.0   0.0  0.0  0.0  0.0  0.324058  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "404634.0   0.0  0.0  0.0  0.0  0.324089  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "           784  785  786  787  788  789  790  791       792  \n",
       "recipe_id                                                    \n",
       "0.0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554025  \n",
       "1.0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554021  \n",
       "5.0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554415  \n",
       "7.0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554030  \n",
       "8.0        0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554028  \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...       ...  \n",
       "404601.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554021  \n",
       "404604.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554020  \n",
       "404606.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554019  \n",
       "404622.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554026  \n",
       "404634.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.554019  \n",
       "\n",
       "[171699 rows x 793 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002713305919337009"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt((recipes.values - decoded.values)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer = beer_ae.decode(np.random.rand(50))\n",
    "random_beer = scaler.inverse_transform(random_beer.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_beer[random_beer < 0.0001] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_ids = np.where(random_beer > 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127, 281, 723, 727, 729, 747, 767, 770, 792])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ing_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeast_american farmhouse blend (wlp670)\n",
      "hop_crystal\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "723",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-ab2939d4ece9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ming_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 723"
     ]
    }
   ],
   "source": [
    "for ing in ing_ids:\n",
    "    print(inv_vocab[ing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas from Ethan\n",
    "\n",
    "\n",
    "## Model Ideas\n",
    "\n",
    "* Look at adding regularization in between layers\n",
    "* Look at VAE\n",
    "* Look at GAN\n",
    "  * This is most interesting\n",
    "  * Its explicit job is creating recipes that look real.\n",
    "  * Discriminator needs to have some function that estimates \"quality\" of what is produced (decoded space).\n",
    "\n",
    "Combining the properties of a VAE (where you can interpolate) and a GAN (where you can estimate \"quality\" of a recipe) would be perfect.\n",
    "\n",
    "Main issue is that reconstruction error doesn't give you the properties you want. Look for other learning signals - what other aspect of a recipe is meaningful? Could we use the beer style as a label?\n",
    "\n",
    "\n",
    "## Encoding Ideas\n",
    "\n",
    "* Can you encode hops into all possible combinations present in the dataset (not all combinations period)? Rob thinks this is reasonable\n",
    "* Can we make ingredient \"categories\"? e.g. caramel with varying lovabond measurement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beer-env",
   "language": "python",
   "name": "beer-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
